{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames Housing Step-by-step - Appendix PCA - Determine the number of PCs to explain 90% of variance in the data\n",
    "\n",
    "Pieter Overdevest  \n",
    "2024-02-09\n",
    "\n",
    "For suggestions/questions regarding this notebook, please contact\n",
    "[Pieter Overdevest](https://www.linkedin.com/in/pieteroverdevest/)\n",
    "(pieter@innovatewithdata.nl).\n",
    "\n",
    "### How to work with this Jupyter Notebook yourself?\n",
    "\n",
    "- Get a copy of the repository ('repo') [machine-learning-with-python-explainers](https://github.com/EAISI/machine-learning-with-python-explainers) from EAISI's GitHub site. This can be done by either cloning the repo or simply downloading the zip-file. Both options are explained in this Youtube video by [Coderama](https://www.youtube.com/watch?v=EhxPBMQFCaI).\n",
    "\n",
    "- Copy the folders 'ames_housing_pieter\\' and 'utils_pieter\\' folder to your own project folder.\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages and assign to a shorter alias.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pieter's utils package.\n",
    "import utils_pieter as up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this appendix, we determine the number of principle components (PC) to explain 90% of the variance in the data using a pipeline ([ref](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)), see also 'ML Explainer' 'pca'. So, we need access to the `explained_variance_ratio_` attribute of a PCA object. As far as I know, this can only be done outside the pipeline. In case we are only interested in the principle components themselves, we can include `PCA(n_components = ...)` in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_impute_scale_numerical = make_pipeline(\n",
    "    \n",
    "    SimpleImputer(missing_values = np.nan, strategy = \"median\"),\n",
    "\n",
    "    StandardScaler()\n",
    "\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is applied to the numerical variables in the original data (`df_orig`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_X_transformed = pl_impute_scale_numerical.fit_transform(df_orig[l_df_X_names])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a PCA object `pca_` anticipating that 25 principle components will cover at least 90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define PCA object.\n",
    "pca_ = PCA(n_components=25)\n",
    "\n",
    "# Matrix containing the principle components.\n",
    "m_pc = pca_.fit_transform(m_X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `explained_variance_ratio_` holds the additional variance that is explained by adding another principle component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pca_ames = pca_.explained_variance_ratio_\n",
    "v_pca_ames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a list comprehension, we demonstrate that 23 PC's are needed to explain at least 90% of the total variance in the data. In other words the 23 PC's contain 90% of the information present in the 38 variables. To show the outcome of `np.cumsum(v_pca_ames)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(l_df_X_names))\n",
    "\n",
    "np.cumsum(v_pca_ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[i,j] for i,j in enumerate(np.cumsum(v_pca_ames))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the following in one table to compare the order of the numerical variables in the data, see table below:\n",
    "\n",
    "1. **Pearson Correlation coefficients** of the numerical variables with `SalePrice`.\n",
    "\n",
    "2. **LASSO coefficients** fitting the numerical variables to a LASSO model predicting `SalePrice`.\n",
    "\n",
    "3. **PCA loadings** of first the principle component ('PC1'). Note, this is independent of `SalePrice`.\n",
    "\n",
    "Two housekeeping remarks:\n",
    "1. We make use of `reset_index(drop=True)` to concatenate the data frames as there are offered. Without it, the data frames would be joined using the original index and all variable names would end up in the same row driven by the index of the first data frame, the correlation coefficients in this case.\n",
    "\n",
    "2. In 'Step 3 - Split the data' in section 'Estimate a LASSO model' two scenario's can be chosen (A and B). Subsequently, in step 6 of the same section, `df_lasso_coefficients` is calculated. To allow for a proper comparison of the correlation coefficients, the LASSO coefficients, and the PCA loadings, please ensure you ran scenario A and steps 3-6 accordingly.\n",
    "\n",
    "The table below allows us to investigate the numerical variable order in each of the three analysis. Comparing the Pearson correlation coefficients and the LASSO coefficients shows that the first two variables occur at the top of each list. We also expect variables that have a high correlation with the `SalePrice` to also end up high in the LASSO coefficient table. *Question: Why?* However, the equality does not continue all the way down. `Garage Area` is high in the list of correlation coefficients, however, it is somewhere in the middle in the list of LASSO coefficients. *Why is this the case?* Since LASSO wants to include as few variables as possible (regularization, remember $\\alpha$) it will choose one of the two. Both are highly correlated (not shown in the table), so the information is sufficiently captured in one of the two.  We observe the same for `Gr Liv Area` and `1st Flr SF`. The table suggests that the variable with the higher correlation with `SalePrice` is chosen for LASSO and the other one is *punished* by giving it a lower LASSO coefficient. So, correlations between variables causes the order in the two lists to differ.\n",
    "\n",
    "We observe no similarity between the order in variables between the loadings in the first principle component (PC1) on the one hand and the correlations and LASSO coefficients on the other hand. Possibly, this is explained by PCA only focussing on the predictor data (X), where correlations and LASSO depend on the relation between the predictor data (X) and `SalePrice` (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_lasso_coefficients.shape[0] != 38:\n",
    "    print(\"Before proceeding, run Scenario A ('numerical only') in Step 3 of section 'Estimate a LASSO model', and run steps 3-6 above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "        \n",
    "    [   # Correlation coefficients. Note, we remove the first row containing correlation of SalePrice with itself.\n",
    "        df_corr_table.tail(-1).reset_index(drop=True),\n",
    "\n",
    "        #LASSO coefficients.\n",
    "        df_lasso_coefficients.reset_index(drop=True),\n",
    "          \n",
    "        # PCA Loadings.  \n",
    "        pd.DataFrame({\n",
    "            \n",
    "            'name': l_df_X_names,\n",
    "            'pc1_loading': [\"{:.2f}\".format(x) for x in pca_.components_[1]],\n",
    "            'pc1_loading_abs': [\"{:.2f}\".format(abs(x)) for x in pca_.components_[1]]\n",
    "\n",
    "        }).sort_values(\n",
    "            \n",
    "            by = 'pc1_loading_abs',\n",
    "            ascending = False\n",
    "            \n",
    "        ).reset_index(drop=True)\n",
    "    ],\n",
    "    \n",
    "    axis = 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebb0a02d86bd6cc81bd76bc6c4cba297e0e4bde90b1df44b0c10ac2ad7a9009a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
