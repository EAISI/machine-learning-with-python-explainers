{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames Housing Step-by-step - Appendix Pipelines - Use Pipelines with LASSO\n",
    "\n",
    "Pieter Overdevest  \n",
    "2024-02-09\n",
    "\n",
    "For suggestions/questions regarding this notebook, please contact\n",
    "[Pieter Overdevest](https://www.linkedin.com/in/pieteroverdevest/)\n",
    "(pieter@innovatewithdata.nl).\n",
    "\n",
    "### How to work with this Jupyter Notebook yourself?\n",
    "\n",
    "- Get a copy of the repository ('repo') [machine-learning-with-python-explainers](https://github.com/EAISI/machine-learning-with-python-explainers) from EAISI's GitHub site. This can be done by either cloning the repo or simply downloading the zip-file. Both options are explained in this Youtube video by [Coderama](https://www.youtube.com/watch?v=EhxPBMQFCaI).\n",
    "\n",
    "- Copy the folders 'ames_housing_pieter\\' and 'utils_pieter\\' folder to your own project folder.\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages and assign to a shorter alias.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pieter's utils package.\n",
    "import utils_pieter as up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we repeat - more or less - what we did above, but now using pipelines to demonstrate their use, see also Python Explainer 'pipelines'. Besides the functions [`Pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) or [`make_pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html), we make use of the [`ColumnTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) function to process numerical and categorical variables differently. We start out by defining separate pipelines for numerical and categorical variables. Here, we use the `Pipeline()` function - instead of the `make_pipeline()` function - so we can reference to the individual transformers later on ([\"Are you using Pipeline in Scikit-Learn?\" by Ankit Goel](https://towardsdatascience.com/are-you-using-pipeline-in-scikit-learn-ac4cd85cb27f)). Imputation is performed by SciKit Learn's [`SimpleImputer()`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting sklearn parameter for Pipeline visualization.\n",
    "import sklearn \n",
    "sklearn.set_config(display=\"diagram\")\n",
    "\n",
    "# Import functions from modules.\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for numerical variables.\n",
    "pl_Num = Pipeline([\n",
    "\n",
    "        ( \"impute\", SimpleImputer(missing_values = np.nan, strategy = \"median\") ),\n",
    "\n",
    "        ( \"scale\",  StandardScaler() )\n",
    "])\n",
    "\n",
    "# Create pipeline for categorical variables.\n",
    "pl_Cat = Pipeline([\n",
    "\n",
    "        ( \"impute\", SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\") ),\n",
    "\n",
    "        ( \"onehot\", OneHotEncoder() )\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `transformers` in the function [`ColumnTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) receives a list of tuples each containing: (1) transformer name, (2) transformer, and (3) variable name(s), resp., see below. Each tuple specifies how the specified variables in the concerned tuple are transformed.\n",
    "\n",
    "Note, `remainder = 'drop'` tells the transformer to drop the variables not mentioned in the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define transformer object.\n",
    "pl_ColumnTransformer = ColumnTransformer(\n",
    "    \n",
    "    transformers = [\n",
    "        \n",
    "        # Tuples containing transformer name, transformer, and variable names for each transformation to take place.\n",
    "        ('num', pl_Num, l_df_X_names),\n",
    "\n",
    "        # As we pull the data through a pipeline, it is easier to add more categorical variables.\n",
    "        ('cat', pl_Cat, ['Neighborhood'] + ['Pool QC'])\n",
    "    ],\n",
    "    \n",
    "    remainder = 'drop',\n",
    "    verbose   = True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with scaling and one-hot encoding we use the `fit_transform()` function to pull a data frame through the pipeline. While with scaling we perform `fit()` and `transform()` separately, to apply the scaling based on the train set to the test set, with one-hot encoding we perform both steps in one go by using `fit_transform()`. The latter approach we apply to the original data, `df_orig`, resulting in `m_X_transformed`.\n",
    "\n",
    "Do the number of columns in `m_X_transformed` correspond to what we expect?\n",
    "\n",
    "Why are we not able to do the calculation for 'Pool QC' in the same notation as for 'Neighborhoord'? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_X_transformed = pl_ColumnTransformer.fit_transform(df_orig)\n",
    "\n",
    "print('')\n",
    "print(f\"Number of columns in the resulting array:          {m_X_transformed.shape[1]}\")\n",
    "print(f\"Number of numerical variables (excl. 'SalePrice'): {len(l_df_X_names)}\")\n",
    "print(f\"Number of unique values in 'Neighborhood':         {len(df_orig['Neighborhood'].value_counts())}\")\n",
    "print(f\"Number of unique values in 'Pool QC':              {len(df_orig['Pool QC'].value_counts())}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next on our list of activities is to create a list of variable names, so we can convert the array `m_X_transformed` to a data frame and assign the corresponding variable names. For the numerical part this is easy, as the numerical variable names are stored in `l_df_X_names`. \n",
    "\n",
    "For the one-hot encoded variables - `Neighborhood` and `Pool QC` - this is a bit trickier. The object `pl_ColumnTransformer` contains the attribute `named_transformers_`. This has two elements, 'num' and 'cat', i.e., the names we gave to the respective transformers. We see the benefit of `Pipeline()` over `make_pipeline()`, since we can make use of the names that we gave to each pipeline. With `make_pipeline()` we do not (have to) give names to the individual transformers, Python creates them for us. However, this means that it is more cumbersome to refer to individual transformers, if needed at later stage, as we see here. See also Python Explainer 'pipeline' in the syllabus.\n",
    "\n",
    "We assign the variable names derived from `pl_ColumnTransformer` to object `v_df_cat_transformed_names`, and we observe that they match what we derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df_cat_transformed_names = pl_ColumnTransformer.named_transformers_['cat']['onehot'].get_feature_names_out(['neighborhood', 'pool_qc'])\n",
    "\n",
    "print(v_df_cat_transformed_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue and append the numeric variable names and those that follow from the two categorical variables, `Neighbordhood` and `Pool QC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df_X_transformed_names = np.append(l_df_X_names, v_df_cat_transformed_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do our checks and balances to confirm the dimensions of our data is mathing our expectations, see below. The transformed data matrix `m_X_transformed` consists of as many columns as there are elements in `v_df_X_transformed_names` that we created from the numerical variables in `df_X` and the unique values in `Neighborhood` and `Pool QC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns in the resulting array: {m_X_transformed.shape[1]}\")\n",
    "print(f\"Length of 'v_df_X_transformed_names':     {len(v_df_X_transformed_names)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to construct data frame `df_X_transformed` that follows from the ColumnTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 2D array to a data frame:\n",
    "df_X_transformed = pd.DataFrame(m_X_transformed, columns = v_df_X_transformed_names)\n",
    "\n",
    "df_X_transformed.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the predictor data using Scikit-Learn's `train_test_split()` ([*ref*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train, df_X_test, ps_y_log_train, ps_y_log_test = f_train_test_split(df_X_transformed, ps_y_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using Scikit Learn's `Lasso()` function to build a single model, we use `LassoCV()` to build a series of LASSO models. By default `LassoCV()` tries 100 different values for $\\alpha$, through the input parameter `n_alphas`. We make use of an example given in SciKit Learn's documentation providing a list of $\\alpha$'s ourselves ([ref](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log of lower border of alphas range.\n",
    "#n_alphas_min = 2\n",
    "\n",
    "mo_lasso = LassoCV(\n",
    "    \n",
    "    # Number of folds.\n",
    "    cv           = 5,\n",
    "\n",
    "    # Fixing random_state ensures the results are reproducible.\n",
    "    random_state = 42,\n",
    "\n",
    "    # Use any CPU available.\n",
    "    #n_jobs       = -1,\n",
    "\n",
    "    # Max number of iterations.\n",
    "    max_iter     = 100000,\n",
    "\n",
    "    # We can enforce for which alphas a Lasso model is fitted.\n",
    "    # In case we do not provide a list, LassoCV() will select 100 values.\n",
    "    #alphas       = [round(10**i) for i in np.arange(n_alphas_min, n_alphas_min+3, 0.2)]\n",
    "\n",
    ").fit(\n",
    "    \n",
    "    df_X_train,\n",
    "    ps_y_log_train\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `mo_lasso` object we can extract the intercept and coefficients of the best model. What explains the number of coefficients equal to zero? How can we increase the number of coefficients equal to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"intercept: {mo_lasso.intercept_:,.0f}\")\n",
    "\n",
    "pd.DataFrame(\n",
    "\n",
    "    {\n",
    "        'coef':     mo_lasso.coef_,\n",
    "        'coef_abs': abs(mo_lasso.coef_),\n",
    "        'variable':  df_X_train.columns\n",
    "    }\n",
    "\n",
    ").sort_values(\n",
    "\n",
    "    'coef_abs',\n",
    "    ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is selected based on the lowest RMSE. The alpha for that model can be obtained through the attribute `alpha_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lowest RMSE found at alpha: {mo_lasso.alpha_:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mo_lasso` object holds the properties of the best model, i.e., the one resulting in the lowest RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_log_pred = mo_lasso.predict(df_X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same three primary metrics are used to evaluate the best LASSO model. The RMSE is considerably lower than when we limited the predictor data to numerical variables only, see above. We observe that by adding `Neighborhood` to the model it can explain a larger part of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.f_evaluate_results(\n",
    "    ps_y_true = ps_y_log_test,\n",
    "    ps_y_pred = ps_y_log_pred\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Appendix B - Determine number of principle components to explain 90% of variance in the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we determine the number of principle components (PC) to explain 90% of the variance in the data using a pipeline ([ref](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)), see also 'ML Explainer' 'pca'. So, we need access to the `explained_variance_ratio_` attribute of a PCA object. As far as I know, this can only be done outside the pipeline. In case we are only interested in the principle components themselves, we can include `PCA(n_components = ...)` in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_impute_scale_numerical = make_pipeline(\n",
    "    \n",
    "    SimpleImputer(missing_values = np.nan, strategy = \"median\"),\n",
    "\n",
    "    StandardScaler()\n",
    "\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is applied to the numerical variables in the original data (`df_orig`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_X_transformed = pl_impute_scale_numerical.fit_transform(df_orig[l_df_X_names])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a PCA object `pca_` anticipating that 25 principle components will cover at least 90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define PCA object.\n",
    "pca_ = PCA(n_components=25)\n",
    "\n",
    "# Matrix containing the principle components.\n",
    "m_pc = pca_.fit_transform(m_X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `explained_variance_ratio_` holds the additional variance that is explained by adding another principle component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pca_ames = pca_.explained_variance_ratio_\n",
    "v_pca_ames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a list comprehension, we demonstrate that 23 PC's are needed to explain at least 90% of the total variance in the data. In other words the 23 PC's contain 90% of the information present in the 38 variables. To show the outcome of `np.cumsum(v_pca_ames)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(l_df_X_names))\n",
    "\n",
    "np.cumsum(v_pca_ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[i,j] for i,j in enumerate(np.cumsum(v_pca_ames))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the following in one table to compare the order of the numerical variables in the data, see table below:\n",
    "\n",
    "1. **Pearson Correlation coefficients** of the numerical variables with `SalePrice`.\n",
    "\n",
    "2. **LASSO coefficients** fitting the numerical variables to a LASSO model predicting `SalePrice`.\n",
    "\n",
    "3. **PCA loadings** of first the principle component ('PC1'). Note, this is independent of `SalePrice`.\n",
    "\n",
    "Two housekeeping remarks:\n",
    "1. We make use of `reset_index(drop=True)` to concatenate the data frames as there are offered. Without it, the data frames would be joined using the original index and all variable names would end up in the same row driven by the index of the first data frame, the correlation coefficients in this case.\n",
    "\n",
    "2. In 'Step 3 - Split the data' in section 'Estimate a LASSO model' two scenario's can be chosen (A and B). Subsequently, in step 6 of the same section, `df_lasso_coefficients` is calculated. To allow for a proper comparison of the correlation coefficients, the LASSO coefficients, and the PCA loadings, please ensure you ran scenario A and steps 3-6 accordingly.\n",
    "\n",
    "The table below allows us to investigate the numerical variable order in each of the three analysis. Comparing the Pearson correlation coefficients and the LASSO coefficients shows that the first two variables occur at the top of each list. We also expect variables that have a high correlation with the `SalePrice` to also end up high in the LASSO coefficient table. *Question: Why?* However, the equality does not continue all the way down. `Garage Area` is high in the list of correlation coefficients, however, it is somewhere in the middle in the list of LASSO coefficients. *Why is this the case?* Since LASSO wants to include as few variables as possible (regularization, remember $\\alpha$) it will choose one of the two. Both are highly correlated (not shown in the table), so the information is sufficiently captured in one of the two.  We observe the same for `Gr Liv Area` and `1st Flr SF`. The table suggests that the variable with the higher correlation with `SalePrice` is chosen for LASSO and the other one is *punished* by giving it a lower LASSO coefficient. So, correlations between variables causes the order in the two lists to differ.\n",
    "\n",
    "We observe no similarity between the order in variables between the loadings in the first principle component (PC1) on the one hand and the correlations and LASSO coefficients on the other hand. Possibly, this is explained by PCA only focussing on the predictor data (X), where correlations and LASSO depend on the relation between the predictor data (X) and `SalePrice` (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_lasso_coefficients.shape[0] != 38:\n",
    "    print(\"Before proceeding, run Scenario A ('numerical only') in Step 3 of section 'Estimate a LASSO model', and run steps 3-6 above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "        \n",
    "    [   # Correlation coefficients. Note, we remove the first row containing correlation of SalePrice with itself.\n",
    "        df_corr_table.tail(-1).reset_index(drop=True),\n",
    "\n",
    "        #LASSO coefficients.\n",
    "        df_lasso_coefficients.reset_index(drop=True),\n",
    "          \n",
    "        # PCA Loadings.  \n",
    "        pd.DataFrame({\n",
    "            \n",
    "            'name': l_df_X_names,\n",
    "            'pc1_loading': [\"{:.2f}\".format(x) for x in pca_.components_[1]],\n",
    "            'pc1_loading_abs': [\"{:.2f}\".format(abs(x)) for x in pca_.components_[1]]\n",
    "\n",
    "        }).sort_values(\n",
    "            \n",
    "            by = 'pc1_loading_abs',\n",
    "            ascending = False\n",
    "            \n",
    "        ).reset_index(drop=True)\n",
    "    ],\n",
    "    \n",
    "    axis = 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebb0a02d86bd6cc81bd76bc6c4cba297e0e4bde90b1df44b0c10ac2ad7a9009a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
