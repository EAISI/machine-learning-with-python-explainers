{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames Housing Step-by-step - Exercise 4\n",
    "\n",
    "Pieter Overdevest  \n",
    "2024-02-09\n",
    "\n",
    "For suggestions/questions regarding this notebook, please contact\n",
    "[Pieter Overdevest](https://www.linkedin.com/in/pieteroverdevest/)\n",
    "(pieter@innovatewithdata.nl).\n",
    "\n",
    "### How to work with this Jupyter Notebook yourself?\n",
    "\n",
    "- Get a copy of the repository ('repo') [machine-learning-with-python-explainers](https://github.com/EAISI/machine-learning-with-python-explainers) from EAISI's GitHub site. This can be done by either cloning the repo or simply downloading the zip-file. Both options are explained in this Youtube video by [Coderama](https://www.youtube.com/watch?v=EhxPBMQFCaI).\n",
    "\n",
    "- Copy the folders 'ames_housing_pieter\\' and 'utils_pieter\\' folder to your own project folder.\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages and assign to a shorter alias.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pieter's utils package.\n",
    "import utils_pieter as up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Explore the outcome variable (`SalePrice`) and how it correlates to other variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding (continued)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the outcome variable 'SalePrice', representing the sale price of the homes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Conduct descriptive/summary statistics on the Y variable (mean, median, std, range)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use the SalePrice data multiple times in this notebook, we assign it (Pandas Series) to an object, which we call, `ps_y_reduced`. What does it mean for the shape of the distribution that the mean exceeds the median?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_reduced = df_reduced['SalePrice']\n",
    "\n",
    "print(\"Summary statistics of sale price (Y):\")\n",
    "print(f\"Mean:   {round(ps_y_reduced.mean(), 1)}\")\n",
    "print(f\"Median: {round(ps_y_reduced.median(), 1)}\")\n",
    "print(f\"Std:    {round(ps_y_reduced.std(), 1)}\")\n",
    "print(f\"Range:  {ps_y_reduced.min()} till {ps_y_reduced.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Plot the distribution of the Y variable. What do we observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use the Vega-Altair library to create visualisations. Vega-Altair is a declarative visualization library for Python, meaning you declare **what** you want to see, instead of providing a step-by-step procedure for detailing how the desired visualisation should be achieved, i.e., imperative programming. The [Getting Started](https://altair-viz.github.io/getting_started/overview.html#overview), [Example Gallery](https://altair-viz.github.io/gallery/index.html#example-gallery), and [Tutorial Exploring Seattle Weather](https://altair-viz.github.io/case_studies/exploring-weather.html) pages on their website are a good place to start. When clicking on an example in the gallery, you will be taken to the source code for that example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package.\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, data distributions are plotted using histograms. We use a [bar chart](https://altair-viz.github.io/gallery/simple_bar_chart.html) to create a histogram of `SalePrice`. The count() function counts the number of rows in each category. The left panel shows the result with the default bin setting. The middle panel shows the result of setting a fixed bin width of $40,000 and the right panel show the result of setting a maximum number of bins to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt1 = alt.Chart(data=df_reduced).mark_bar().encode(\n",
    "    x = 'SalePrice',\n",
    "    y = 'count()'\n",
    ")\n",
    "\n",
    "alt2 = alt.Chart(data=df_reduced).mark_bar().encode(\n",
    "    alt.X('SalePrice', bin=alt.Bin(step=40000)),\n",
    "    alt.Y('count()', title='Count')\n",
    ")\n",
    "\n",
    "alt3 = alt.Chart(data=df_reduced).mark_bar().encode(\n",
    "    alt.X('SalePrice', bin=alt.Bin(maxbins=16)),\n",
    "    alt.Y('count()', title='Count')\n",
    ")\n",
    "\n",
    "alt1 | alt2 | alt3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the data has a right-skewed distribution. This explains why the mean `SalePrice` is larger than its median. This is a phenomenon that occurs more often in nature and society, where a variable cannot be negative and does not have a maximum on the positive side. Yearly income is another example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra - How to obtain an estimation for bin width and bin number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin width and bin number are a means to steer the granularity of the histogram. What is the downside of having too few and of having too many bins?\n",
    "\n",
    "In the middle panel in the figure above, we assumed a bin width of $40,000. Given the distribution of the observed data we can calculate a bin width using the [Freedman–Diaconis rule](https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule):\n",
    "\n",
    "$$\n",
    "bin\\,width = \\frac{2 * IQR(x)} {\\sqrt[3]{x}}\n",
    "$$\n",
    "\n",
    "where, IQR(*x*) is the InterQuartile Range of sample *x*, and *n* is number of observations in the sample. The number of bins can be derived simply by dividing the range by the bin width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IQR is equal to the difference between the 75th and 25th percentiles.\n",
    "n_q25, n_q75 = np.percentile(ps_y_reduced, q = [25,75])\n",
    "\n",
    "n_IQR = n_q75 - n_q25\n",
    "\n",
    "# The bin width is calculated using the Freedman-Diaconis rule, see above.\n",
    "n_bin_width = 2 * n_IQR / (len(ps_y_reduced)**(1/3))\n",
    "\n",
    "# The number of bins easily follows from the range and the bin width.\n",
    "n_bins = int((max(ps_y_reduced) - min(ps_y_reduced))/n_bin_width)\n",
    "\n",
    "print(f\"Freedman–Diaconis bin width:      {n_bin_width:,.0f}\")\n",
    "print(f\"Freedman–Diaconis number of bins: {n_bins}\")\n",
    "print(f\"\\nThis also means that 50% of the houses were sold between ${n_q25:,.0f} (n_q25) and ${n_q75:,.0f} (n_q75).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Investigate how `Gr Liv Area` (numerical) relates to the Y variable. Tip: see Altair's [scatter plot](https://altair-viz.github.io/gallery/scatter_tooltips.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we investigate how the `Gr Liv Area` variable - as example of a numerical variable - and the `SalePrice` variable (response) relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the numerical feature we want to investigate.\n",
    "c_num = \"Gr Liv Area\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c1. Scatter plot of `Gr Liv Area` against `SalePrice`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [scatter plot](https://altair-viz.github.io/gallery/scatter_tooltips.html) to plot the sale price against the grand living area for each house that was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot - Part I\n",
    "alt.Chart(df_reduced).mark_circle().encode(\n",
    "    x = c_num,\n",
    "    y = 'SalePrice'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tune the same figure a bit.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot - Part II\n",
    "alt.Chart(df_reduced).mark_circle(size=60).encode(\n",
    "    x       = c_num,\n",
    "    y       = 'SalePrice',\n",
    "    color   = 'Neighborhood',\n",
    "    tooltip = [c_num, 'SalePrice', 'Neighborhood']\n",
    ").properties(\n",
    "    height = 400,\n",
    "    width  = 600\n",
    ").configure_axis(\n",
    "    labelFontSize = 15,\n",
    "    titleFontSize = 15\n",
    ").configure_mark(\n",
    "    opacity = 0.5\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the theme by returning the dictionary of configurations.\n",
    "# https://altair-viz.github.io/user_guide/customization.html\n",
    "def f_theme_altair():\n",
    "\n",
    "    return {\n",
    "        'config': {\n",
    "            'view': {\n",
    "                'height': 400,\n",
    "                'width':  600,\n",
    "            },\n",
    "            'axis': {\n",
    "                'labelFontSize': 15,\n",
    "                'titleFontSize': 15,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Register the custom theme under a chosen name.\n",
    "alt.themes.register('my_theme_altair', f_theme_altair)\n",
    "\n",
    "# Enable the newly registered theme.\n",
    "alt.themes.enable('my_theme_altair');\n",
    "\n",
    "# If you want to restore the default theme, use:\n",
    "#alt.themes.enable('default');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can reproduce the figure above with less code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot - Part III\n",
    "alt.Chart(df_reduced).mark_circle(size=60).encode(\n",
    "    x       = c_num,\n",
    "    y       ='SalePrice',\n",
    "    color   = 'Neighborhood',\n",
    "    tooltip = [c_num, 'SalePrice', 'Neighborhood']\n",
    ").configure_mark(\n",
    "    opacity = 0.5\n",
    ")#.interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better appreciation for the data density and the general trend, we use a [2D histogram heatmap](https://altair-viz.github.io/gallery/histogram_heatmap.html) to bin the data into a grid and display the count of each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_reduced).mark_rect().encode(\n",
    "    alt.X(c_num).bin(step=100),\n",
    "    alt.Y('SalePrice').bin(step=25000),\n",
    "    alt.Color('count()').scale(scheme='greenblue')\n",
    ")#.interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c2. Faceted scatter plot of `Gr Liv Area` against `SalePrice`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the length of the legend and the broad spectrum of colours in the scatter plots above, it is clear that this approach is not very informative beyond a general trend. In the next plot we make use of the `facet()` method. Reducing the alpha helps to understand the distribution of the data and whether there really is a trend or not. By default - with alpha equal to 1 - ten overlapping data points give the same impression as one data point.\n",
    "\n",
    "The figure below suggests that houses sold in the more expensive areas - like 'NridgHt' and 'StoneBr' - have a higher sale price per square feet of `Gr Liv Area`, than in the less expensive areas, like 'Sawyer' and 'SWISU'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_reduced).mark_circle().encode(\n",
    "    x = c_num,\n",
    "    y = 'SalePrice',\n",
    ").properties(\n",
    "    height = 200,\n",
    "    width  = 200\n",
    ").facet(\n",
    "    'Neighborhood',\n",
    "    columns = 5\n",
    ")#.interactive()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Investigate how `Neighborhood` (categorical) relates to the Y variable. Tip: see Altair's [histogram](https://altair-viz.github.io/gallery/simple_histogram.html) and [boxplot](https://altair-viz.github.io/gallery/boxplot.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we investigate how the `Neighborhood` variable - as example of a categorical variable - and the `SalePrice` variable (response) relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the categorical feature we want to investigate.\n",
    "c_cat = \"Neighborhood\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d1. Number of neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at the relation between neighborhoods and saleprice, let's see how many houses have been sold in each neighborhood. We will plot these numbers in a histogram, ordering the neighborhoods by the median saleprice in the respective neighborhood.\n",
    "\n",
    "For that we create a list of the 28 neighborhoods ordered by the median saleprice in the respective neighborhoods. The list suggests that MeadowV has the lowest median value for `SalePrice` and StoneBr has the highest median value for `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cat_ordered = (\n",
    "\n",
    "    df_reduced\n",
    "    .groupby([c_cat])\n",
    "    ['SalePrice']\n",
    "    .median()\n",
    "    .sort_values()\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(l_cat_ordered, \"\\n\")\n",
    "print(f\"{c_cat} with lowest median saleprice:  {l_cat_ordered[0]}\")\n",
    "print(f\"{c_cat} with highest median saleprice: {l_cat_ordered[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [bar chart](https://altair-viz.github.io/gallery/simple_bar_chart.html) and use the count() function to count the number of rows in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data=df_reduced).mark_bar().encode(\n",
    "    x=alt.X(c_cat, sort=l_cat_ordered),\n",
    "    y=alt.Y('count()', title = 'Count')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe no clear relation between the neighborhood and the number of houses sold in said neighborhood. In NAmes close to 450 houses were sold, where in Landmark and GrnHill hardly any houses were sold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d2. Distribution of Y variable per neighborhood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the facet() method to plot the distribution of the Sale Price in each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_reduced).mark_bar().encode(\n",
    "    x=alt.X(\n",
    "        'SalePrice',\n",
    "        bin  = alt.Bin(step=50000),\n",
    "        axis = alt.Axis(format='~s')\n",
    "    ),\n",
    "    y=alt.Y(\n",
    "        'count()',\n",
    "        title = 'Count'\n",
    "\n",
    "    )\n",
    ").properties(\n",
    "    height = 150,\n",
    "    width  = 150\n",
    ").facet(\n",
    "    c_cat,\n",
    "    columns = 4\n",
    ").resolve_scale(\n",
    "    y = 'independent' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe different distributions of sale price among the neighborhoods. In NAmes and Saywer the distribution is narrower than in other neighborhoods. In StoneBr and NridgHt the distribution is wider than in other neighborhoods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d3. Box plot of Y variable per neighborhood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Box plots](https://altair-viz.github.io/gallery/boxplot.html) allow for a different - more compact - representation of distributions.  As before, the neighborhoods are ordered by `l_cat_ordered`. As expected, we observe the median value - the horizontal line in the middle of each box - goes up steadily from left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data=df_reduced).mark_boxplot().encode(\n",
    "    x = alt.X(c_cat, sort=l_cat_ordered),\n",
    "    y = alt.Y('SalePrice'),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d4. Strip plot of Y variable per Neighborhood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give another way to visualise the distribution, we can use the [strip plot](https://altair-viz.github.io/gallery/strip_plot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_reduced).mark_tick().encode(\n",
    "    x = alt.Y(c_cat, sort=l_cat_ordered),\n",
    "    y = alt.X('SalePrice')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (continued)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Assess the distribution of `SalePrice` in exercise 4b. What did you observe? What does it mean for the performance of the prediction model? Log-transform the outcome variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that the `SalePrice` distribution is right-skewed. This means there is no symmetric distribution around the estimated values. This causes the expensive homes in Q4 to 'pull' the predictions to higher values, than the homes in Q1 can prevent. To solve this, we apply log transformation making the distribution more like a normal (symmetric) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create 'log' brother of ps_y_reduced.\n",
    "ps_y_reduced_log = np.log(ps_y_reduced)\n",
    "\n",
    "# Add ps_y_log to df_reduced.\n",
    "df_reduced['SalePrice_log'] = ps_y_reduced_log\n",
    "\n",
    "# Add 'SalePrice_log' to l_df_num_names.\n",
    "l_df_num_names.append('SalePrice_log')\n",
    "\n",
    "# Let's see the distribution ps_y_log. Looks much better!\n",
    "alt1 = alt.Chart(data=df_reduced).mark_bar().encode(\n",
    "    alt.X('SalePrice', bin=alt.Bin(step=12000)),\n",
    "    alt.Y('count()', title='Count')\n",
    ").properties(\n",
    "    title='Original SalePrice distribution'\n",
    ")\n",
    "\n",
    "alt2 = alt.Chart(data=df_reduced).mark_bar().encode(\n",
    "    alt.X('SalePrice_log', bin=alt.Bin(step=0.1)),\n",
    "    alt.Y('count()', title='Count')\n",
    ").properties(\n",
    "    title='Log-transformed SalePrice distribution'\n",
    ")\n",
    "\n",
    "alt1 | alt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we observe the log-transformed SalePrice to have a more symmetric normal distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. Assess `Gr Liv Area` for all houses in the previous exercise. What do you observe? Remove outliers. What does it mean for the scope of the prediction model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exercise 4d, we observed five houses that have an exceptionally large grand living area (`Gr Liv Area`), while three of them even have a relatively low sale price. We remove all houses with `Gr Liv Area` exceeding 4,000 sq ft, and limit the scope of the model to houses having `Gr Liv Area` up to 4,000 sq ft.\n",
    "\n",
    "We proceed the remainder of the analysis with `df_scoped`. It is good practice to assign a new variable name to the new object, since it concerns a significant change. This keeps `df_reduced` available for later reference.  Of course, you cannot create new objects for each step/change, you will need to find the right balance, between RAM and clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations with 'Gr Liv Area' exceeding 4000 sq ft. In addition,\n",
    "# we reset the index to prevent issues later on in case of merging data.\n",
    "df_scoped = df_reduced.query(\"`Gr Liv Area` < 4000\").reset_index(drop=True)\n",
    "\n",
    "# We create a reduced version of ps_y_log.\n",
    "ps_y_scoped_log = df_scoped.SalePrice_log\n",
    "ps_y_scoped     = df_scoped.SalePrice\n",
    "\n",
    "# And we create reduced versions of the numerical and categorical data.\n",
    "df_scoped_num = df_scoped.select_dtypes(include='number').reset_index(drop=True)\n",
    "df_scoped_cat = df_scoped.select_dtypes(include='category').reset_index(drop=True)\n",
    "\n",
    "print(\n",
    "    f\"We proceed the analysis with {df_scoped.shape[0]} observations out \"\n",
    "    f\"of the {df_reduced.shape[0]} observations in the original dataset.\\n\"\n",
    "    f\"The data frame has {df_scoped.shape[1]} columns, of which \"\n",
    "    f\"{df_scoped_num.shape[1]} are numerical and \"\n",
    "    f\"{df_scoped_cat.shape[1]} are categorical.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt1 = alt.Chart(df_reduced).mark_circle().encode(\n",
    "    x = c_num,\n",
    "    y = 'SalePrice'\n",
    ").properties(\n",
    "    title='Original data (df_reduced)'\n",
    ")\n",
    "\n",
    "alt2 = alt.Chart(df_scoped).mark_circle().encode(\n",
    "    x = c_num,\n",
    "    y = 'SalePrice'\n",
    ").properties(\n",
    "    title='Scoped data (df_scoped)'\n",
    ")\n",
    "\n",
    "alt1 | alt2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding (continued)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g. Draw scatter plots between Y and each of the numerical features. Tip: see Altair's [scatter plot](https://altair-viz.github.io/gallery/scatter_tooltips.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we created a list object `l_df_num_names` containing all numerical variable names in the data. We will use this object to create scatter plots of each numerical variable against the SalePrice. We define `l_df_X_names` as subset of `l_df_num_names` containing all variables except `SalePrice` and `SalePrice_log`. To accomplish that we make use of a list comprehension ([RealPython](https://realpython.com/list-comprehension-python/)), see also the Python Explainer 'List Comprehensions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df_X_names = [x for x in l_df_num_names if x not in ['SalePrice', 'SalePrice_log']]\n",
    "\n",
    "print(len(l_df_X_names), l_df_X_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to create a 'chart row' (i.e., a row of charts) using the [repeat() method](https://altair-viz.github.io/user_guide/compound_charts.html#repeated-charts). As input arguments it takes a data frame and a list of variable names present in the data frame. The respective variables are plotted against `SalePrice_log`. To get a better understanding of the position of the majority of the data, it helps to set a low opacity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_create_chart_row(df_data, l_col):\n",
    "\n",
    "    return (\n",
    "        alt.Chart(df_data)\n",
    "        .mark_point(opacity=0.1)\n",
    "        .encode(\n",
    "            x=alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "            y=alt.Y(alt.repeat(\"row\"), type='quantitative')\n",
    "        )\n",
    "        .properties(width=200, height=200)\n",
    "        .repeat(\n",
    "            column = l_col,\n",
    "            row    = ['SalePrice_log']            \n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of chart rows, each chart row contains `n_col` charts. We create as many chart rows until each of the 38 charts has its place in a chart row. In the example below we set `n_col` to four. This means that we end up with ten chart rows. The first nine chart rows each hold four charts. The tenth chart row holds the remaining two charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of charts per row of chart row.\n",
    "n_col = 4\n",
    "\n",
    "# Create list of chart rows (i.e., row of charts) each having n_col charts.\n",
    "l_chart_row = [\n",
    "    \n",
    "    # Create chart row.\n",
    "    f_create_chart_row(\n",
    "        df_data = df_scoped,\n",
    "        l_col   = l_df_X_names[i:i+n_col]\n",
    "    )\n",
    "    \n",
    "    for i in range(0, len(l_df_X_names), n_col)\n",
    "]\n",
    "\n",
    "# To explain what 'range(0, len(l_df_X_names), n_col))' holds:\n",
    "print(list(range(0, len(l_df_X_names), n_col)))\n",
    "\n",
    "# First item in 'l_chart_row':\n",
    "l_chart_row[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the 38 charts on a canvas. Note, the '*' operator is used to unpack a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt.vconcat(*l_chart_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that `Overall Qual`, `Garage Cars`, and `Garage Area` seem to correlate well with `SalePrice`. The variable `Total Bsmt SF` also correlates well with `SalePrice`, while we also see some outliers that may cause the trendline to become shallower. Also with other 'SF' variables we observe outliers; something to keep in mind for Data Preparation. What does it mean for the prediction model that some variables correlate well with the outcome variable Y?\n",
    "\n",
    "In case you want to include a trendline in each of the scatterplots, check out the function below, which is part of the utils_pieter package. The .repeat() method does not seem to work together with the .transform_regression() method, see scrapyard.py in the example-solutions\\archive\\ folder. So, we create a list of 38 single charts (incl. trendline) using f_plot_scatter_with_trend(). Then, the list is converted to a list of row charts, which is plotted using vconcat() and hconcat() using f_plot_scatter_with_trend_grid()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up.f_plot_scatter_with_trend_grid(\n",
    "#     df_input = df_scoped,\n",
    "#     l_x      = l_df_X_names,\n",
    "#     c_y      = 'SalePrice_log',\n",
    "#     n_col    = 4\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h. Create a table showing the Pearson correlation coefficients between Y and each of the numerical variables. Tip: see [pearsonr()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson correlation coefficients between the outcome variable (Y) and each of the numerical variables quantifies how well they correlate to each other.\n",
    "\n",
    "The Pearson correlation coefficient - named after [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson) - quantifies the strength of the linear relationship between two numerical data samples. To understand the calculation of Pearson's correlation coefficient we construct the calculation using the formula below ([ref](https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/)) and we compare it to the outcome of the `pearsonr()` function. The Pearson coefficient is calculated as,\n",
    "\n",
    "$$\n",
    "Pearson\\,correlation\\,coefficient = \\frac{covariance(X, Y)} {std(X) * std(Y)}\n",
    "$$\n",
    "\n",
    "For correlation between categorical and numerical variables you can use ANOVA or the Point Biseral Test ([ref](https://www.tutorialspoint.com/correlation-between-categorical-and-continuous-variables)). For correlation between categorical variables you can use Cramer's V (symmetrical) or Theil's U (asymmetrical) ([ref](https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9))."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### h1. - How is the Pearson correlation coefficient calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will work through an example where we calculate the Pearson correlation coefficients between the variables `SalePrice_log` and `Overall Qual`, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Initialize.\n",
    "c_variable = \"Overall Qual\"\n",
    "#c_variable = \"Gr Liv Area\"\n",
    "#c_variable = \"Garage Cars\"\n",
    "\n",
    "# Extract variable data from the data frame.\n",
    "ps_variable    = df_scoped[c_variable]\n",
    "\n",
    "# Calculate covariance matrix.\n",
    "m_cov          = np.cov(ps_variable, ps_y_scoped_log)\n",
    "\n",
    "# Covariance matrix - Calculate Pearson correlation coefficient between two variables.\n",
    "n_corr_cov_mat = round(m_cov[0,1] / (np.std(ps_y_scoped_log) * np.std(ps_variable)), 3)\n",
    "\n",
    "# Pearsonr() - Calculate Pearson correlation coefficient between two variables.\n",
    "n_corr_pearson = round(pearsonr(ps_y_scoped_log, ps_variable)[0], 3)\n",
    "\n",
    "print(f\"Pearson correlation coefficient between variable '{c_variable}' and 'SalePrice_log', calculated using:\")\n",
    "\n",
    "print(f\"Covariance matrix-based formula: {n_corr_cov_mat}\")\n",
    "\n",
    "print(f\"Python built-in function:        {n_corr_pearson}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### h2. Calculate Pearson correlation coefficients between all numerical variables, incl Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_pearson_corr = [\n",
    "\n",
    "    round(pearsonr(ps_y_scoped_log, df_scoped[x])[0], 3)\n",
    "\n",
    "    for x in l_df_num_names\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `l_pearson_corr` contains the Pearson correlation coefficients between `SalePrice_log` and each numerical variable. We will use it to construct a data frame sorted by the absolute value of the Pearson correlation coefficient in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_table = (\n",
    "\n",
    "    pd.DataFrame({'name': l_df_num_names, 'corr': l_pearson_corr})\n",
    "    .assign(corr_abs = lambda row: abs(row['corr']))\n",
    "    .sort_values(by = 'corr_abs', ascending=False)\n",
    ")\n",
    "\n",
    "df_corr_table.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kept `SalePrice` and `SalePrice_log` in the data, so we could observe what correlation values would be calculated. Why do we observe 1 for `SalePrice_log`? and 0.95 for `SalePrice`?\n",
    "\n",
    "We observe that - of the other variables - `Overall Qual` has the highest correlation with `SalePrice_log`. What does this mean in case of a model predicting `SalePrice_log`?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Create correlation plots showing the correlations between each pair of numerical variables, incl. Y. Tip: see Seaborn's [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) and [Fritz' Blog](https://fritz.ai/seaborn-heatmaps-13-ways-to-customize-correlation-matrix-visualizations/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmaps are a useful way to visualize correlations. Seaborn has a straightforward solution for plotting heatmaps of correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i1. Plot correlation heatmap for all numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the benefit of this approach, we plot a heatmap of all correlations among all numerical variables. We can identify which pairs have high and which pairs have low correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10)) \n",
    "\n",
    "sns.heatmap(\n",
    "    data   = df_scoped_num.corr(),\n",
    "    annot  = False,\n",
    "    square = True,\n",
    "    cmap   = 'coolwarm'\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i2. Plot correlation heatmap for Top-10 numerical variables having the highest correlation with Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10)) \n",
    "\n",
    "sns.heatmap(\n",
    "    data      = df_scoped_num[df_corr_table.head(10)['name']].corr(),\n",
    "    annot     = True,\n",
    "    square    = True,\n",
    "    annot_kws = {\"size\": 12},\n",
    "    cmap      = 'coolwarm'\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the confirmation of the correlation between `SalePrice_log` and the other numerical variables, we also observe that `Garage Area` and `Garage Cars` are highly correlated. What does this mean in terms of our model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i4. Plot correlation heatmap for the numerical variables having 'SF' in the variable name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another way of subsetting the numerical variables. Suppose we want to select only those variables that contain 'SF' ('square feet')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df_num_names_sf = [x for x in l_df_num_names if \"SF\" in x or x == 'SalePrice_log']\n",
    "\n",
    "plt.figure(figsize=(10, 10)) \n",
    "\n",
    "sns.heatmap(\n",
    "    data      = df_scoped_num[l_df_num_names_sf].corr(),\n",
    "    annot     = True,\n",
    "    square    = True,\n",
    "    annot_kws = {\"size\": 12},\n",
    "    cmap      = 'coolwarm'\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows that `Total Bsmt SF` and `1st Flr SF` are highly correlated. What does this mean in terms of our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, I made a function `f_heatmap()`, see `utils_pieter` package to see the source code. Feel free to copy paste the source code into your notebook and check out how it works.  See also Altair's [simple heatmap plot](https://altair-viz.github.io/gallery/simple_heatmap.html) and [Annual Weather Heatmap example](https://altair-viz.github.io/gallery/annual_weather_heatmap.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_pieter import f_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_heatmap(    \n",
    "    df_input      = df_scoped_num,\n",
    "    l_df_names    = df_corr_table.head(10)['name'],\n",
    "    b_add_corr    = True,\n",
    "    n_font_size   = 14,\n",
    "    n_canvas_size = 400\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebb0a02d86bd6cc81bd76bc6c4cba297e0e4bde90b1df44b0c10ac2ad7a9009a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
