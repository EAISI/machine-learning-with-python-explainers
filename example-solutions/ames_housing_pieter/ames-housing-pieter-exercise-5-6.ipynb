{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames Housing Step-by-step - Exercise 5 and 6\n",
    "\n",
    "Pieter Overdevest  \n",
    "2024-02-09\n",
    "\n",
    "For suggestions/questions regarding this notebook, please contact\n",
    "[Pieter Overdevest](https://www.linkedin.com/in/pieteroverdevest/)\n",
    "(pieter@innovatewithdata.nl).\n",
    "\n",
    "### How to work with this Jupyter Notebook yourself?\n",
    "\n",
    "- Get a copy of the repository ('repo') [machine-learning-with-python-explainers](https://github.com/EAISI/machine-learning-with-python-explainers) from EAISI's GitHub site. This can be done by either cloning the repo or simply downloading the zip-file. Both options are explained in this Youtube video by [Coderama](https://www.youtube.com/watch?v=EhxPBMQFCaI).\n",
    "\n",
    "- Copy the folders 'ames_housing_pieter\\' and 'utils_pieter\\' folder to your own project folder.\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages and assign to a shorter alias.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pieter's utils package.\n",
    "import utils_pieter as up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Estimate a Linear Regression, a LASSO and a kNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models that we will develop in this section originate from the [SciKit-Learn](https://scikit-learn.org/stable/) library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the development of a linear regression model ([sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and ['Linear Regression in Python' by Dannar Mawardi](https://towardsdatascience.com/linear-regression-in-python-9a1f5f000606))."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Estimate a Linear Regression model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through a number of steps, starting with defining a list holding all numerical variables excluding `SalePrice` and `SalePrice_log`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a1. Recap our objects and define a few more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we defined data frames, `df_scoped_num` and `df_scoped_cat`, and the lists holding their respective variable names, `l_df_num_names` and `l_df_cat_names`. We also defined the list `l_df_X_names` as a subset of `l_df_num_names`, excluding `SalePrice` and `SalePrice_log`. Below, we define data frame `df_X` containing the data of the variable in `l_df_X_names`.\n",
    "\n",
    "To confirm we did our book keeping right, we confirm that: (1) `l_df_X_names` is two items shorter than `l_df_num_names`, namely `SalePrice` and `SalePrice_log` and (2) `l_df_num_names` and `l_df_cat_names` make up all variables in the updated and reduced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of l_df_X_names:          {len(l_df_X_names)}\")\n",
    "print(f\"Length of l_df_num_names:        {len(l_df_num_names)}\")\n",
    "print(f\"Length of l_df_cat_names:        {len(l_df_cat_names)}\")\n",
    "print(f\"Number of columns in df_reduced: {len(df_reduced.columns)}\")\n",
    "print(f\"Number of columns in df_reduced: {len(df_scoped.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the scenario of interest add/remove the hash signs in front of the variable names. Note, `df_reduced` contains all observations that we reduced in memory size by converting the string type data to the categorical type and downcasted the numerical data to their smallest container, and in `df_scoped` we dropped some of the houses that had `Gr Liv Area` > 4000 sq ft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data, ps_y_data_log = df_scoped.copy(), ps_y_scoped_log.copy()\n",
    "#df_data, ps_y_data_log = df_reduced.copy(), ps_y_reduced_log.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `df_X` as the data frame holding all numerical variables - except `SalePrice` and `SalePrice_log` - that we will use to predict the Y variable (`SalePrice_log`). You may wonder, will we use categorical variables as predictors? Good question, the short answer is yes, more will come later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_data[l_df_X_names]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start simple we build a linear model considering the Top-4 numerical variables with the highest correlation with `SalePrice_log`, the variable that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_df_X_names_subset = ['Overall Qual']\n",
    "#l_df_X_names_subset = ['Overall Qual', 'Gr Liv Area']\n",
    "#l_df_X_names_subset = ['Overall Qual', 'Gr Liv Area', 'Garage Cars']\n",
    "l_df_X_names_subset = ['Overall Qual', 'Gr Liv Area', 'Garage Cars', 'Garage Area']\n",
    "#l_df_X_names_subset = ['Overall Qual', 'Gr Liv Area', 'Total Bsmt SF']\n",
    "\n",
    "# Define object 'df_X_subset' as subset of df_X.\n",
    "df_X_subset = df_X[l_df_X_names_subset]\n",
    "\n",
    "print(df_X.shape, df_X_subset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a2. Draw scatter plot and correlation plot of the predictors (`l_df_X_names_subset`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know what we are working with, we plot the concerned variables against `SalePrice_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_data).mark_point(opacity=0.1).encode(\n",
    "    x = alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    y = alt.Y(alt.repeat(\"row\"), type='quantitative')\n",
    ").properties(\n",
    "    width  = 200,\n",
    "    height = 200\n",
    ").repeat(\n",
    "    column = l_df_X_names_subset,\n",
    "    row    = ['SalePrice_log']            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5)) \n",
    "\n",
    "sns.heatmap(\n",
    "    data      = df_data[l_df_X_names_subset + ['SalePrice_log']].corr(),\n",
    "    annot     = True,\n",
    "    square    = True,\n",
    "    annot_kws = {\"size\": 12},\n",
    "    cmap      = 'coolwarm'\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a3. Split the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the subset of the predictor data (`df_X_subset` and `ps_y_data_log`) using Scikit-Learn's `train_test_split()` ([*ref*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data in train and test sets.\n",
    "df_X_train, df_X_test, ps_y_log_train, ps_y_log_test = train_test_split(\n",
    "    \n",
    "    df_X_subset,\n",
    "    ps_y_data_log,\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `f_train_test_split()` in the utils_pieter package does the same as Scikit-Learn's `train_test_split()` function, in addition, it prints some stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train and test sets.\n",
    "# df_X_train, df_X_test, ps_y_log_train, ps_y_log_test = up.f_train_test_split(\n",
    "    \n",
    "#     df_X_subset,\n",
    "#     ps_y_data_log,\n",
    "#     n_test_size=0.33\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a4. Scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in `df_X_subset` is standardized so that each variable is represented at the same scale. Generally, there are two ways to scale variable data ([ref](https://www.analyticsvidhya.com/blog/2020/04/variable-scaling-machine-learning-normalization-standardization/)):\n",
    "\n",
    "* Normalization is a scaling technique in which values are translated and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n",
    "\n",
    "* Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation ([ref](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)). This means that the mean of the variable becomes zero and the resultant distribution has a unit standard deviation.\n",
    "\n",
    "There is no rule to tell you what to choose when. You can always start by fitting your model to raw, normalized and standardized data, resp., and evaluate the three outcomes. In the example below, we will standardize the variable data so that each variable will have μ = 0 and σ = 1.\n",
    "\n",
    "Scaling of outcome variable is generally not required. Only when the outcome variable is not normally distributed, like is our case here, scaling will help to improve model performance.\n",
    "\n",
    "It is a good practice to 'fit' (apply) the scaler object on the training data and then use the same scaler object to transform the testing data. This would avoid any data leakage during the model testing process. Therefore, we use scikit-learn's `fit()` and `transform()` functions in subsequent steps, even though scikit-learn also has a `fit_transform()` function that does both in one go. The difference between `fit()` and `transform()`:\n",
    "1. 'fit' applies a transformer, like scaling or encoding. The result is an updated 'machine', that knows *how* to convert an input to an output, e.g. from orginal data to scaled data. In case of standardization it knows the mean and the standard deviation of the data it was fit on.\n",
    "2. 'transform' actually transforms input data to output data. The updated machine is used to convert the input to the output.\n",
    "\n",
    "The downside of `fit_transform()` is that in case we apply if before to the train/test split, we leak information from the test data into the training data ([ref](https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe)). If we apply it after the train/test split we introduce different means and standard deviations in both train and test data. Applying `fit()` to the train data allows us to use the resulting mean and standard deviation to standardize the test data, applying `transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaling object.\n",
    "scaler = StandardScaler()\n",
    " \n",
    "# Standardization of variables in training data.\n",
    "scaler_fitted_on_X_train = scaler.fit(df_X_train)\n",
    "\n",
    "# Key properties of scaler object.\n",
    "pd.DataFrame({'name': l_df_X_names_subset, 'mean': scaler_fitted_on_X_train.mean_, 'std': scaler_fitted_on_X_train.scale_})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `scaler` object - that was 'fitted' on the variables in the training data - in our hands, we can apply it to transform the  variables in the training data as well as to transform the variables in the test data. This approach avoids any information leakage from the test data into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train_scaled = pd.DataFrame(\n",
    "    scaler_fitted_on_X_train.transform(df_X_train),\n",
    "    columns = l_df_X_names_subset\n",
    ")\n",
    "\n",
    "df_X_test_scaled  = pd.DataFrame(\n",
    "    scaler_fitted_on_X_train.transform(df_X_test),\n",
    "    columns = l_df_X_names_subset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_X_train_scaled).mark_bar().encode(\n",
    "    x = alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    y = \"count()\"\n",
    ").properties(\n",
    "    width  = 200,\n",
    "    height = 200,\n",
    "    title = \"Standardized training data\"\n",
    ").repeat(\n",
    "    column = l_df_X_names_subset,          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_X_test_scaled).mark_bar().encode(\n",
    "    x = alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    y = \"count()\"\n",
    ").properties(\n",
    "    width  = 200,\n",
    "    height = 200,\n",
    "    title = \"Standardized test data\"\n",
    ").repeat(\n",
    "    column = l_df_X_names_subset,          \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a5. Train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed all preparations, we can train the linear regression model on the train data. It starts by defining `mo_lin_reg` as an 'empty' linear regression model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mo_lin_reg = LinearRegression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What attributes are available in the 'empty' model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_lin_reg.__dict__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill it by fitting the model on the train data. We update the object by fitting the model on the train data (input/output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_lin_reg.fit(df_X_train_scaled, ps_y_log_train);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use `__dict__` to see what attributes are available in the `mo_lin_reg` object. Now, we observe the presence of the regression coefficients (`coef_`) and the intercept (`intercept_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_lin_reg.__dict__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a6. Interpret the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a general trend that the larger the correlation with `SalePrice_log` the larger the fitted coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Intercept: {mo_lin_reg.intercept_:,.2f}\\n\")\n",
    "\n",
    "print(\"Coefficients:\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    'variable': l_df_X_names_subset,    \n",
    "    'coeff':   [str(round(x,3)) for x in mo_lin_reg.coef_],\n",
    "    'corr':    df_corr_table.query('name in @l_df_X_names_subset')['corr']\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a7. Make predictions based on estimated model and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model - present in `mo_line_reg` - is used to make `SalePrice_log` predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_log_pred = mo_lin_reg.predict(df_X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a8. Evaluate estimated model based on test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three primary metrics that can be used to evaluate regression models:\n",
    "\n",
    "* **Mean Absolute Error (MAE):** The easiest to understand. Represents average error.\n",
    "\n",
    "* **Mean Squared Error (MSE):** Similar to MAE but noise is exaggerated and larger errors result in higher “punishment”, as the error is squared. MSE is harder to interpret than MAE as it’s not in base units, however, it is generally more popular.\n",
    "\n",
    "* **Root Mean Squared Error (RMSE):** Most popular metric, and being the square root of MSE - the RMSE is better interpretable as it has the same unit as the outcome variable (Y). This makes RMSE the recommended metric to interpret your linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will evaluate these metrics more often, `f_evaluate_results()` has been defined, as part of 'utils_pieter'. This function also returns a scatter plot of the predicted vs actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.f_evaluate_results(\n",
    "    ps_y_true = ps_y_log_test,\n",
    "    ps_y_pred = ps_y_log_pred\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Estimate a LASSO model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO (Least Absolute Shrinkage and Selection Operator) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting machine learning model ([ref](https://machinelearningmastery.com/lasso-regression-with-python/)). Recall that for regularized linear regression a cost function is added. In case of LASSO this is:\n",
    "\n",
    "$$ J(\\Theta) = MSE (\\Theta) + \\alpha\\sum\\limits_{i=1}^n \\mid{\\Theta_{i}}\\mid$$\n",
    "\n",
    "The larger the hyperparameter $\\alpha$ - sometimes referred to as $\\lambda$ - the larger the penalty and the more coefficients will be set to zero, resulting in a model based on fewer variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### b1. Recap our objects and define a few more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the numerical variables in the data (`df_X`) you may also want to include categorical data to predict the `SalePrice`. To give an example, earlier, we observed that `Neighborhood` correlated well with `SalePrice`. This can be done by so-called 'One-Hot Encoding' ([ref1](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/), [ref2](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd)). In the example below we add the `Neighborhood` variable to the predictor data. One-hot encoding adds new variables to the data, one for each unique value and entering a '1' for observations where it had the concerned value and a '0' in all other cases.\n",
    "\n",
    "You can see how the `fit_transform()` function is also used here. As in case of scaling, we could perform 'fit' and 'transform' separately. To simplify we do them both in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define a one-hot encoder object.\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# The fit_transform() function produces a 'sparse matrix' (sm_) object. To convert the data to\n",
    "# a data frame we first need to convert it to a matrix (m_).\n",
    "sm_neighborhoods = ohe.fit_transform(df_data[['Neighborhood']])\n",
    "m_neighborhoods  = sm_neighborhoods.toarray()\n",
    "\n",
    "# The one-hot encoder object 'ohe' has been updated by applying the fit_transform() function to it.\n",
    "# Now it has the 'categories_' attribute. Since it is an array in a list we take the first element.\n",
    "# Note, we convert the default floats to integers to save memory (more for demo purposes, since our\n",
    "# dataset is relatively small already).\n",
    "df_X_ohe = pd.DataFrame(\n",
    "    data    = m_neighborhoods,\n",
    "    columns = ohe.categories_[0]\n",
    ").astype('int')\n",
    "\n",
    "# Comms to the user. The created data frame has as many variables as there are unique values in the\n",
    "# 'Neighborhood' variable in df_data and the same number of observations as df_data has.\n",
    "# No surprises.\n",
    "print(f\"Unique neighborhoods: {ohe.categories_[0]}\\n\")\n",
    "\n",
    "print(f\"Length of the 'categories_' attribute:         {len(ohe.categories_[0])}\")\n",
    "print(f\"Number of unique neighborhoods in df_data:     {len(df_data['Neighborhood'].unique())}\\n\")\n",
    "\n",
    "print(f\"Dimensions of created data frame ('df_X_ohe'): {df_X_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two dataframes horizontally, as before, set 'axis' to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_combined = pd.concat([df_X, df_X_ohe], axis = 1)\n",
    "\n",
    "# Comms to the user. The created data frame 'df_X_combined' has as many observations as both 'X' and 'X_ohe' have,\n",
    "# and as many columns as the two have together. \n",
    "print(\"\\nThe sum of the number of columns in df_X and df_X_ohe must equal that in df_X_combined:\")\n",
    "print(f\"Number of variables in df_X:          {df_X.shape[1]}\")\n",
    "print(f\"Number of variables in df_X_ohe:      {df_X_ohe.shape[1]}\")\n",
    "print(f\"Number of variables in df_X_combined: {df_X_combined.shape[1]}\")\n",
    "\n",
    "print(\"\\nThe number of rows must all be the same:\")\n",
    "print(f\"Number of rows in df_X:               {df_X.shape[0]}\")\n",
    "print(f\"Number of rows in df_X_ohe:           {df_X_ohe.shape[0]}\")\n",
    "print(f\"Number of rows in df_X_combined:      {df_X_combined.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b2. Visualize predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show a sample of the combined data frame to clarify how one-hot coding works and how the result was added to the numerical data. In which neighborhoods are the first five houses located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_combined.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b3. Split the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can follow two scenarios:\n",
    "\n",
    "Scenario A - Only use the numerical variables (df_X), so excluding the one-hot encoded `Neighborhood` variable.\n",
    "\n",
    "Scenario B - Use the combined data (df_X_combined), so including the one-hot encoded `Neighborhood` variable.\n",
    "\n",
    "Depending on what scenario you want to follow, add/remove the '#'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario A.\n",
    "#df_X_train, df_X_test, ps_y_log_train, ps_y_log_test = up.f_train_test_split(df_X, ps_y_data_log)\n",
    "\n",
    "# Scenario B.\n",
    "df_X_train, df_X_test, ps_y_log_train, ps_y_log_test = up.f_train_test_split(df_X_combined, ps_y_data_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b4. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we standardize `df_X_train` and `df_X_test` separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaling object.\n",
    "scaler = StandardScaler()\n",
    " \n",
    "# Standardization of variables in train data.\n",
    "scaler_fitted_on_X_train = scaler.fit(df_X_train)\n",
    "\n",
    "# Key properties of scaler object for first 10 variables in df_X_train.\n",
    "pd.DataFrame({'name': df_X_train.columns, 'mean': scaler_fitted_on_X_train.mean_, 'sd': scaler_fitted_on_X_train.scale_}).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train_scaled = pd.DataFrame(\n",
    "    scaler_fitted_on_X_train.transform(df_X_train),\n",
    "    columns = df_X_train.columns\n",
    ")\n",
    "\n",
    "df_X_test_scaled  = pd.DataFrame(\n",
    "    scaler_fitted_on_X_train.transform(df_X_test),\n",
    "    columns = df_X_test.columns\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the scaled data. In the one-hot encoded neighborhood variables we observe that the 0's and 1's have been replaced by a negative and a positive number, resp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain why the 0's and 1's have been transformed to negative and positive numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dummy list of 0's and 1's.\n",
    "l_temp = np.zeros(2).tolist()+np.ones(3).tolist()\n",
    "print(l_temp)\n",
    "\n",
    "# We standardize these numbers.\n",
    "(l_temp - np.mean(l_temp)) / np.std(l_temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b5. Train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using Scikit Learn's `Lasso()` function to build a single model ([ref](https://scikit-learn.org/stable/modules/linear_model.html#lasso)), we use `LassoCV()` to build a series of LASSO models ([ref](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)). By default `LassoCV()` tries 100 different values for $\\alpha$ (`n_alphas`=100 by default). Another way to control alpha is by providing a list of alpha's to evaluate; this is what we do here. We set the min and max value of the alpha range and use these values to calculate the step size in order to have 100 alpha values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log of lower border of alphas range.\n",
    "n_alphas_min_log = np.log(0.0001)\n",
    "n_alphas_max_log = np.log(0.01)\n",
    "n_step = (n_alphas_max_log - n_alphas_min_log) / 100\n",
    "\n",
    "# List of alphas.\n",
    "l_alpha = [round(np.e**i,5) for i in np.arange(n_alphas_min_log, n_alphas_max_log, n_step)]\n",
    "\n",
    "# First 10 elements.\n",
    "print(l_alpha[:10])\n",
    "\n",
    "# Last 10 elements.\n",
    "print(l_alpha[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "mo_lasso = LassoCV(\n",
    "    \n",
    "    # Number of folds.\n",
    "    cv           = 5,\n",
    "\n",
    "    # Fixing random_state ensures the results are reproducible.\n",
    "    random_state = 42,\n",
    "\n",
    "    # Max number of iterations.\n",
    "    max_iter     = 1000,\n",
    "\n",
    "    # We can enforce for which alphas a Lasso model is fitted.\n",
    "    # In case we do not provide a list, LassoCV() will select 100 values.\n",
    "    alphas       = l_alpha\n",
    "\n",
    ").fit(\n",
    "    \n",
    "    df_X_train_scaled,\n",
    "    ps_y_log_train\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b6. Interpret the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the folds we plot the RMSE against the respective $\\alpha$'s we added as parameter to `LassoCV()`, see colored dotted lines in the figure below. We also add the mean of the folds, see black line. This plot allows us to choose the optimal $\\alpha$, i.e., the one that results in the lowest RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option(\"display.precision\", 5)\n",
    "\n",
    "df_lasso = (\n",
    "    \n",
    "    pd.DataFrame(np.sqrt(mo_lasso.mse_path_))\n",
    "    .rename(columns   = {i : f\"rmse_{i+1}\" for i in range(5)})\n",
    "    .assign(rmse_mean = np.sqrt(mo_lasso.mse_path_.mean(axis=-1)))\n",
    "    .assign(alpha     = mo_lasso.alphas_)\n",
    "    .sort_values(by = 'alpha')\n",
    "    .reset_index(drop = True)\n",
    ")\n",
    "\n",
    "# Transform the data frame to long format.\n",
    "df_lasso_long = pd.melt(df_lasso, id_vars = 'alpha', var_name = 'fold', value_name = 'rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is selected based on the lowest RMSE. The alpha for that model can be obtained through the attribute `alpha_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We observe the lowest RMSE {min(df_lasso.rmse_mean):.3f} at alpha: {mo_lasso.alpha_:,.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_chart = alt.Chart(df_lasso_long).mark_line().encode(\n",
    "    x     = alt.X('alpha', scale = alt.Scale(type='log'), title=\"Alpha\"),\n",
    "    y     = alt.Y('rmse',  scale = alt.Scale(domain=[0.1, 0.16])),\n",
    "    color = alt.Color(\n",
    "        'fold',\n",
    "        scale = alt.Scale(\n",
    "            domain = [f\"rmse_{i+1}\" for i in range(5)] + ['rmse_mean'],\n",
    "            range  = ['red']*5 + ['black']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical dotted line at alpha = mo_lasso.alpha_\n",
    "vertical_dotted_line = alt.Chart(df_lasso_long).mark_rule(\n",
    "        color      = 'red',\n",
    "        strokeDash = [5,5]\n",
    "    ).encode(\n",
    "        x = alt.X('a:Q', title=\"\")\n",
    "    ).transform_calculate(\n",
    "        a = str(mo_lasso.alpha_)\n",
    "    )\n",
    "\n",
    "# Combine the scatter plot and the vertical dotted line\n",
    "base_chart + vertical_dotted_line\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `mo_lasso` object we can extract the intercept and coefficients of the best model. What explains the number of coefficients equal to zero? How can we increase the number of coefficients equal to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"intercept: {mo_lasso.intercept_:,.2f}\")\n",
    "\n",
    "df_lasso_coefficients = pd.DataFrame(\n",
    "\n",
    "    {\n",
    "        'name':            df_X_train_scaled.columns,\n",
    "        'lasso coeff':     mo_lasso.coef_,\n",
    "        'lasso_coeff_abs': abs(mo_lasso.coef_)        \n",
    "    }\n",
    "\n",
    ").sort_values(\n",
    "\n",
    "    'lasso_coeff_abs',\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.min_rows\", 40)\n",
    "\n",
    "df_lasso_coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b7. Make predictions based on estimated model and test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mo_lasso` object holds the properties of the best model, i.e., the one resulting in the lowest RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_y_log_pred = mo_lasso.predict(df_X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b8. Evaluate estimated model based on test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same three primary metrics are used to evaluate the best LASSO model. The RMSE is considerably lower than with the linear regression model, however, we require a lot more variables. Using $\\alpha$ we can reduce the number of included variables, but this goes at the 'expense' of a higher RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.f_evaluate_results(\n",
    "    ps_y_true = ps_y_log_test,\n",
    "    ps_y_pred = ps_y_log_pred\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Estimate a kNN model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we develop a kNN model to predict the `SalePrice_log` ([ref1](https://realpython.com/knn-python/), [ref2](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)). We define a KNeighborsRegressor object, called `mo_knn`, that is further informed by fitting the training data. Contrary to (LASSO) regression, we keep this section on kNN simple and to the point; you got the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Define KNN object.\n",
    "mo_knn = KNeighborsRegressor(n_neighbors = 2)\n",
    "\n",
    "mo_knn.fit(df_X_train_scaled, ps_y_log_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained kNN model is used to predict `SalePrice` for the train and test data. We do this to investigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.f_evaluate_results(\n",
    "    ps_y_true = ps_y_log_train,\n",
    "    ps_y_pred = mo_knn.predict(df_X_train_scaled)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.f_evaluate_results(\n",
    "    ps_y_true = ps_y_log_test,\n",
    "    ps_y_pred = mo_knn.predict(df_X_test_scaled)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For low values of `n_neighbors` the model is highly flexible and we are running the risk of over-fitting. Observing the RMSE value for the train and test set, we can conclude that we are indeed overfitting. We can make use of SciKit Learn's `GridSearchCV()` function to work through a series of hyperparameters, and determine for which hyperparameter we found the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\"n_neighbors\": range(1, 50)}\n",
    "gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)\n",
    "\n",
    "gridsearch.fit(df_X_train_scaled, ps_y_log_train)\n",
    "\n",
    "# Comms to the user\n",
    "print(f\"We found the best model for {gridsearch.best_params_}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.f_evaluate_results(\n",
    "    ps_y_true = ps_y_log_train,\n",
    "    ps_y_pred = gridsearch.predict(df_X_train_scaled)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.f_evaluate_results(\n",
    "    ps_y_true = ps_y_log_test,\n",
    "    ps_y_pred = gridsearch.predict(df_X_test_scaled)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, no over-fitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 - Assess which model performs best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is discussed with each of the models. I leave making the overall assessment to you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebb0a02d86bd6cc81bd76bc6c4cba297e0e4bde90b1df44b0c10ac2ad7a9009a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
